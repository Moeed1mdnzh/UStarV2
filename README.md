# UStarV2 - A Step Ahead
UStarV2 is an improved version of [UStarV1](https://github.com/Moeed1mdnzh/UStarV1) in which we aimed to increase the qualities of the following features
- Resolution
- Evaluation Metric
- Data
- Model Complexity
- Optimization
Unlike the previous version, we used pytorch instead of tensorflow and keras for the implemetantion since we needed a faster, more efficient and flexible model.
Here are a couple of examples generated by UStarV2


|Input|Prediction| 
|-------------|-------------|
|![](https://github.com/Moeed1mdnzh/UStarV2/blob/master/previews/sample_1.jpg)|![](https://github.com/Moeed1mdnzh/UStarV2/blob/master/previews/prediction_1.jpg)|
![](https://github.com/Moeed1mdnzh/UStarV2/blob/master/previews/sample_2.jpg)|![](https://github.com/Moeed1mdnzh/UStarV2/blob/master/previews/prediction_2.jpg)|
![](https://github.com/Moeed1mdnzh/UStarV2/blob/master/previews/sample_3.jpg)|![](https://github.com/Moeed1mdnzh/UStarV2/blob/master/previews/prediction_3.jpg)|
![](https://github.com/Moeed1mdnzh/UStarV2/blob/master/previews/sample_4.jpg)|![](https://github.com/Moeed1mdnzh/UStarV2/blob/master/previews/prediction_4.jpg)|
  ## Resolution
  In the previous version our dataset consisted of 128x128 images though in the updated dataset, our image sizes are increased to 192x192 yet we have provided the [ESRGAN](https://esrgan.readthedocs.io/en/latest/)
  super-resoluting model which is capable of providing us with 768x768 results. Here's a demonstration of the model<br />
  ![](https://github.com/Moeed1mdnzh/UStarV2/blob/master/assets/Demonstration.png)
## Evaluation Metric
Besides generator and discriminator losses, we also added the [Frechet Inception Distance](https://en.wikipedia.org/wiki/Fr%C3%A9chet_inception_distance) which is a similarity function that we implemented through numpy.
Keep in mind that we didn't base our model optimization on FID.
## Data
Our dataset was made with the help of a single [image](https://github.com/Moeed1mdnzh/UStarV2/blob/master/dataset/data.jpg) which was designed by [anco_artwork](https://www.instagram.com/anco_artwork?igshid=ODA1NTc5OTg5Nw==).
With the help of image processing techniques we turned this single image into roughly 43000 images resulting in a dataset with the size of 43008x192x192x3
## Model Complexity
In UStarV1 we had a pretty basic UNet model and a path gan.In the current version we heavily modified both models in terms of complexity, regularization, activaion functions and minibatchstddev.If you're willing
to seek more information about the model, Take a look the model folder.
## Optimization
Fortunately, the selection of the optimization of the previous model, was set to perfect so we used the same hyperparameters here.

# How To Generate
## Installation
### Clone
Open up command prompt or terminal and use the following command to clone this repo<br />
`git clone https://github.com/Moeed1mdnzh/UStar-GUI.git`<br />
### Install the requirements
```python
pip install -r requirements.txt 
```
The installation of pytorch is not mentioned since it differs in terms of installation.You need to visit [https://pytorch.org](https://pytorch.org) and find the compatible version for your machine
## Train The Model
**If you're willing to use the pretrained model, skip this part.**<br />
If you don't want to train the model on your very own machine, you can do the same following steps on google colab.
The configurations of the model such as the optimizer, loss function, epochs, batch size and so on and so forth are defined in the ***configs.py*** file in which every variable is written in a simple and understandable
way.<br />
If you want to get deeper in the modification of the hyperparameters, you may want to take a look at the **model** folder in which the generator and discriminator are separately configured.<br />
The model was trained for 5 epochs to give the current results, however you may get better results if trained longer but its not guaranteed.<br />
Finally run the following command to have the data generated<br />
```python
python data_generator.py
```
<br />
Now after the generation of the data, The time of the training has arrived.With no effort at all run the following and the loggs will guide you throughout the training<br /><br />

```python
python train.py
```

<br />
The generator and discriminator of each epoch will be saved in a new folder.The predictions of each epoch will also be saved as well as the evaluations as a json file.

## Download the pretrained model
Make a folder named *pre_trained_models* folder in the main folder.Next use the following commands to download the UStarV2 and ESRGAN model from google drive<br />
`gdown 1--h-AjyyNUU0SeBN3k63cKWD7P8NHnJP`<br />
`gdown 1-3l2lLlEE4K9Dl8bnFNBIa9DbmDit3Z1`<br />
and put both models in the *pre_trained_models* folder.
## Run
Simply type in<br />
`python main.py`<br />
![](https://github.com/Moeed1mdnzh/UStarV2/blob/master/assets/page_1.JPG)<br />
This is the screen you must face after running the main file.<br />
Choose your pen size from the slider on the top.<br />Choose your desired color from the color palette on the left.<br />The main tools are illustrated on the left side of the screen(brush and eraser) with the exception of a third tool.With the help of that tool
you are able to brush scatteredly.<br />The buttons on the right are obvious as you expect.If you activate **Super-Resolute** the program decides wether to use the ESRGAN model or not.Here's the outcome screen<br />
![](https://github.com/Moeed1mdnzh/UStarV2/blob/master/assets/page_2.JPG)<br /> 
I suppose in this page, everything is obvious.The saved image, can be found in the main folder with the name ***result.png***
